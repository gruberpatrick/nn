<link rel="stylesheet" type="text/css" href="./css/main.css">
<h1 id="introduction">Introduction</h1>
<h2 id="intiuition-and-math">Intiuition and math:</h2>
<p>If we consider a classification problem with the following data:</p>
<figure>
<img src="https://docs.microsoft.com/en-us/cognitive-toolkit/tutorial/synth_data.png" title="Example data" alt="Picture 1: Example data for classification." /><figcaption>Picture 1: Example data for classification.</figcaption>
</figure>
<p>We have two input dimensions <code>x1</code> and <code>x2</code>, as well as output labels <code>y</code>. Our job is to classify a given input of arbitrary values <code>x1</code> and <code>x2</code>, and assign them to class <code>0</code> or <code>1</code>.</p>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png" title="Neural Network representation." alt="Picture 2: Neural Network representation." /><figcaption>Picture 2: Neural Network representation.</figcaption>
</figure>
<p>Let’s take a look a the following neural network (NN):</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}PARAMS%20%3D%20%5BX%2C%20Y%2C%20W_1%2C%20b_1%2C%20W_2%2C%20b_2%5D%20%5C%5C%20Z_1%20%3D%20W_1%2AX%20%2B%20b_1%20%5C%5C%20A_1%20%3D%20%5Csigma%28Z_1%29%20%5C%5C%20Z_2%20%3D%20W_2%20%2A%20A_1%20%2B%20b_2%20%5C%5C%20A_2%20%3D%20%5Csigma%28Z_2%29%20%5C%5C%20L%28A_2%2C%20Y%29%20%3D%3E%20%5Chat%7By%7D%20%5C%5C%20RETURNS%20%3D%20%5BZ_1%2C%20A_1%2C%20Z_2%2C%20A_2%2C%20L%28A_2%2C%20Y%29%2C%20%5Chat%7By%7D%5D" alt="PARAMS = [X, Y, W_1, b_1, W_2, b_2] \\ Z_1 = W_1*X + b_1 \\ A_1 = \sigma(Z_1) \\ Z_2 = W_2 * A_1 + b_2 \\ A_2 = \sigma(Z_2) \\ L(A_2, Y) =&gt; \hat{y} \\ RETURNS = [Z_1, A_1, Z_2, A_2, L(A_2, Y), \hat{y}]" title="PARAMS = [X, Y, W_1, b_1, W_2, b_2] \\ Z_1 = W_1*X + b_1 \\ A_1 = \sigma(Z_1) \\ Z_2 = W_2 * A_1 + b_2 \\ A_2 = \sigma(Z_2) \\ L(A_2, Y) =&gt; \hat{y} \\ RETURNS = [Z_1, A_1, Z_2, A_2, L(A_2, Y), \hat{y}]" /><br /></p>
<p><em>Note that to simplify the calculations the input now only has one input feature <code>X</code>.</em></p>
<p>A typical NN consists of multiple layers of neurons that will make decisions, based on provided data and parameters. The parameters <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}X" alt="X" title="X" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}Y" alt="Y" title="Y" /> are part of the problem set. <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}X" alt="X" title="X" /> contains multiple input values that help the NN approach the label value <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}Y" alt="Y" title="Y" />, where the output of the NN is typically referred to as <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Chat%7By%7D" alt="\hat{y}" title="\hat{y}" />. The difference between <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}Y" alt="Y" title="Y" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Chat%7By%7D" alt="\hat{y}" title="\hat{y}" /> will determine the loss of the NN. Our goal is to find parameter values for <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W_1%2C%20b_1%2C%20W_2%2C%20b_2" alt="W_1, b_1, W_2, b_2" title="W_1, b_1, W_2, b_2" /> that minimize the loss as much as possible.</p>
<p>In the above NN, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Csigma" alt="\sigma" title="\sigma" /> is an activation function to make the output of a layer non-linear (we will use sigmoid), and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}L" alt="L" title="L" /> is a loss function that satisfies the given problem (in our case cross entropy).</p>
<p>As a side note, these two links should help understand why activation functions are neccessary:</p>
<ul class="incremental">
<li><a href="https://www.wolframalpha.com/input/?i=.6%28.5*x+%2B+.2%29+%2B+.1">https://www.wolframalpha.com/input/?i=.6%28.5*x+%2B+.2%29+%2B+.1</a></li>
<li><a href="https://www.wolframalpha.com/input/?i=.6%28sigmoid%28.5*x+%2B+.2%29%29%2B.1">https://www.wolframalpha.com/input/?i=.6%28sigmoid%28.5*x+%2B+.2%29%29%2B.1</a></li>
</ul>
<p>In our initial NN, all of these operations are chained together, meaning that <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Chat%7By%7D" alt="\hat{y}" title="\hat{y}" /> could also be represented as one long equation. An algorithm called Backpropagation can be used to calculate the gradients of all learnable parameters <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%28W_1%2C%20b_1%2C%20...%29" alt="(W_1, b_1, ...)" title="(W_1, b_1, ...)" />. This will give us the contribution of each parameter to the overall loss. Gradient Descent is used to minimize the overall loss to a (local) minimum.</p>
<h2 id="backpropagation">Backpropagation</h2>
<figure>
<img src="https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png" title="Gradient Descent" alt="Picture 3: The image shows us how we move from a start position to a local minimum by adjusting our parameters, one step at a time." /><figcaption>Picture 3: The image shows us how we move from a start position to a local minimum by adjusting our parameters, one step at a time.</figcaption>
</figure>
<p>To be able to determine how we can walk downhill using Gradient Descent, we have to know what the slope (or gradient) of our current position is.</p>
<figure>
<img src="https://sebastianraschka.com/images/faq/closed-form-vs-gd/ball.png" title="Slope using Gradient Descent" alt="Picture 4: The slope of the current function at a given point." /><figcaption>Picture 4: The slope of the current function at a given point.</figcaption>
</figure>
<p>The simplest way to calculate the slope is by using the formula <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}b%20%3D%20%5Cfrac%7B%28x%20-%20x%60%29%28y%20-%20y%60%29%7D%7B%28x%20-%20x%60%29%5E2%7D" alt="b = \frac{(x - x`)(y - y`)}{(x - x`)^2}" title="b = \frac{(x - x`)(y - y`)}{(x - x`)^2}" /><br />, which requires us to have two points on the slope. In our problem we have a multi dimensional landscape, and it is therefore almost impossible to find two points that would help us nagivate “down hill”. Calculus thought us that the derivative of a function gives us the slope at a specific point. In other words, we don’t have to find multiple points to get the gradient. The complexity of the required derivative increases with the depth of the NN. Luckily, because all the operations of the NN are subfunctions, we can use the chain rule to solve for desired gradients: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7BdW%7D%20%3D%20%5Cfrac%7BdL%7D%7BdA%7D%2A%5Cfrac%7BdA%7D%7BdZ%7D%2A%5Cfrac%7BdZ%7D%7BdW%7D." alt="\frac{dL}{dW} = \frac{dL}{dA}*\frac{dA}{dZ}*\frac{dZ}{dW}." title="\frac{dL}{dW} = \frac{dL}{dA}*\frac{dA}{dZ}*\frac{dZ}{dW}." /><br /></p>
<p>To give this some more context, let’s calculate the derivative of the loss function <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}L" alt="L" title="L" /> with respect to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}A_2" alt="A_2" title="A_2" />. We’ll use cross entropy as our loss function for this example: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7BdA_2%7D%5B-%5Cfrac%7B1%7D%7Bm%7D%5Csum%5E%7Bm%7D_%7Bi%3D0%7D%28Y%2Alog%28A_2%29%2B%281-Y%29%2Alog%281-A_2%29%29%5D%20%3D%20%5Ccolor%7Bred%7D%7B-%5Cfrac%7BY%7D%7BA_2%7D%2B%5Cfrac%7B1-Y%7D%7B1-A_2%7D%7D." alt="\frac{dL}{dA_2}[-\frac{1}{m}\sum^{m}_{i=0}(Y*log(A_2)+(1-Y)*log(1-A_2))] = \color{red}{-\frac{Y}{A_2}+\frac{1-Y}{1-A_2}}." title="\frac{dL}{dA_2}[-\frac{1}{m}\sum^{m}_{i=0}(Y*log(A_2)+(1-Y)*log(1-A_2))] = \color{red}{-\frac{Y}{A_2}+\frac{1-Y}{1-A_2}}." /><br /></p>
<p>Next up we have to calculate the derivative of our activation function. For our example, we’ll use the sigmoid function, as it limits the values to a range of [0, 1]: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdA_2%7D%7BdZ_2%7D%5B%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Z_2%7D%7D%5D%20%5C%5C%20%3D%20%5Cfrac%7BdA%7D%7BdZ%7D%5B%281%2Be%5E%7B-Z_2%7D%29%5E%7B-1%7D%5D%20%5C%5C%20%3D%20-%281%2Be%5E%7B-Z_2%7D%29%5E%7B-2%7D%28-e%5E%7B-Z_2%7D%29%20%5C%5C%20%3D%20%5Cfrac%7Be%5E%7B-Z_2%7D%7D%7B%281%2Be%5E%7B-Z_2%7D%29%5E2%7D%20%5C%5C%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Z_2%7D%7D%5Cfrac%7B1%2Be%5E%7B-Z_2%7D-1%7D%7B1%2Be%5E%7B-Z_2%7D%7D%20%5C%5C%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Z_2%7D%7D%281-%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Z_2%7D%7D%29%20%5C%5C%20%3D%20%5Ccolor%7Bred%7D%7BA_2%281-A_2%29%7D" alt="\frac{dA_2}{dZ_2}[\frac{1}{1+e^{-Z_2}}] \\ = \frac{dA}{dZ}[(1+e^{-Z_2})^{-1}] \\ = -(1+e^{-Z_2})^{-2}(-e^{-Z_2}) \\ = \frac{e^{-Z_2}}{(1+e^{-Z_2})^2} \\ = \frac{1}{1+e^{-Z_2}}\frac{1+e^{-Z_2}-1}{1+e^{-Z_2}} \\ = \frac{1}{1+e^{-Z_2}}(1-\frac{1}{1+e^{-Z_2}}) \\ = \color{red}{A_2(1-A_2)}" title="\frac{dA_2}{dZ_2}[\frac{1}{1+e^{-Z_2}}] \\ = \frac{dA}{dZ}[(1+e^{-Z_2})^{-1}] \\ = -(1+e^{-Z_2})^{-2}(-e^{-Z_2}) \\ = \frac{e^{-Z_2}}{(1+e^{-Z_2})^2} \\ = \frac{1}{1+e^{-Z_2}}\frac{1+e^{-Z_2}-1}{1+e^{-Z_2}} \\ = \frac{1}{1+e^{-Z_2}}(1-\frac{1}{1+e^{-Z_2}}) \\ = \color{red}{A_2(1-A_2)}" /><br /> To show the principle of the chain rule, let’s calculate <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7BdZ_2%7D" alt="\frac{dL}{dZ_2}" title="\frac{dL}{dZ_2}" />: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7BdZ_2%7D%20%3D%20%5Cfrac%7BdL%7D%7BdA_2%7D%20%2A%20%5Cfrac%7BdA_2%7D%7BdZ_2%7D%20%5C%5C%20%3D%20%28-%5Cfrac%7BY%7D%7BA_2%7D%2B%5Cfrac%7B1-Y%7D%7B1-A_2%7D%29%28A_2%29%281-A_2%29%20%5C%5C%20%3D%20%28-Y%2B%5Cfrac%7BA_2%281-Y%29%7D%7B%281-A_2%29%7D%29%281-A_2%29%20%5C%5C%20%3D%20-Y%281-A_2%29%2BA_2%281-Y%29%20%5C%5C%20%3D%20%5Ccolor%7Bred%7D%7BA_2-Y%7D." alt="\frac{dL}{dZ_2} = \frac{dL}{dA_2} * \frac{dA_2}{dZ_2} \\ = (-\frac{Y}{A_2}+\frac{1-Y}{1-A_2})(A_2)(1-A_2) \\ = (-Y+\frac{A_2(1-Y)}{(1-A_2)})(1-A_2) \\ = -Y(1-A_2)+A_2(1-Y) \\ = \color{red}{A_2-Y}." title="\frac{dL}{dZ_2} = \frac{dL}{dA_2} * \frac{dA_2}{dZ_2} \\ = (-\frac{Y}{A_2}+\frac{1-Y}{1-A_2})(A_2)(1-A_2) \\ = (-Y+\frac{A_2(1-Y)}{(1-A_2)})(1-A_2) \\ = -Y(1-A_2)+A_2(1-Y) \\ = \color{red}{A_2-Y}." /><br /></p>
<p>Now we’ll start the gradient calculation of our first parameter, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W_2" alt="W_2" title="W_2" />: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdZ_2%7D%7BdW_2%7D%5BW_2%2AA_1%20%2B%20b_2%5D%20%3D%20%5Ccolor%7Bred%7D%7BA_1%7D%2C" alt="\frac{dZ_2}{dW_2}[W_2*A_1 + b_2] = \color{red}{A_1}," title="\frac{dZ_2}{dW_2}[W_2*A_1 + b_2] = \color{red}{A_1}," /><br /> which means that for the contribution of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W_2" alt="W_2" title="W_2" /> <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7BdW_2%7D%20%3D%20%5Cfrac%7BdL%7D%7BdZ%7D%20%2A%20%5Cfrac%7BdZ%7D%7BdW_2%7D%20%3D%20%20%5Ccolor%7Bred%7D%7B%28A_2-Y%29A_1%7D." alt="\frac{dL}{dW_2} = \frac{dL}{dZ} * \frac{dZ}{dW_2} =  \color{red}{(A_2-Y)A_1}." title="\frac{dL}{dW_2} = \frac{dL}{dZ} * \frac{dZ}{dW_2} =  \color{red}{(A_2-Y)A_1}." /><br /></p>
<p>Similarly we also have to know <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7Bdb_2%7D" alt="\frac{dL}{db_2}" title="\frac{dL}{db_2}" />: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdZ_2%7D%7Bdb_2%7D%5BW_2%2AA_1%20%2B%20b_2%5D%20%3D%20%5Ccolor%7Bred%7D%7B1%7D%2C" alt="\frac{dZ_2}{db_2}[W_2*A_1 + b_2] = \color{red}{1}," title="\frac{dZ_2}{db_2}[W_2*A_1 + b_2] = \color{red}{1}," /><br /> and again <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7Bdb_2%7D%20%3D%20%5Cfrac%7BdL%7D%7BdZ%7D%20%2A%20%5Cfrac%7BdZ%7D%7Bdb_2%7D%20%3D%20%5Ccolor%7Bred%7D%7B%28A_2-Y%29%7D." alt="\frac{dL}{db_2} = \frac{dL}{dZ} * \frac{dZ}{db_2} = \color{red}{(A_2-Y)}." title="\frac{dL}{db_2} = \frac{dL}{dZ} * \frac{dZ}{db_2} = \color{red}{(A_2-Y)}." /><br /></p>
<p>In theory these would be all the calculations needed, if we only had one layer in our NN. Because we also have to calculate the gradients for <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W_1" alt="W_1" title="W_1" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}b_1" alt="b_1" title="b_1" />, we need to first calculate the gradients for <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}A_1" alt="A_1" title="A_1" />, since they propagate through the same function as <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W_2" alt="W_2" title="W_2" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}b_2" alt="b_2" title="b_2" />. We have already done the required work for this: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdZ_2%7D%7BdA_1%7D%5BW_2%2AA_1%20%2B%20b_2%5D%20%3D%20%5Ccolor%7Bred%7D%7BW_2%7D%2C" alt="\frac{dZ_2}{dA_1}[W_2*A_1 + b_2] = \color{red}{W_2}," title="\frac{dZ_2}{dA_1}[W_2*A_1 + b_2] = \color{red}{W_2}," /><br /> which means that <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7BdA_1%7D%20%3D%20%5Cfrac%7BdL%7D%7BdZ%7D%20%2A%20%5Cfrac%7BdZ_2%7D%7BdA_1%7D%20%3D%20%5Ccolor%7Bred%7D%7B%28A_2-Y%29W_2%7D." alt="\frac{dL}{dA_1} = \frac{dL}{dZ} * \frac{dZ_2}{dA_1} = \color{red}{(A_2-Y)W_2}." title="\frac{dL}{dA_1} = \frac{dL}{dZ} * \frac{dZ_2}{dA_1} = \color{red}{(A_2-Y)W_2}." /><br /></p>
<p>Now we continue in the same fashion as we did for <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W_2" alt="W_2" title="W_2" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}b_2" alt="b_2" title="b_2" />, and will be able to obtain <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W_1" alt="W_1" title="W_1" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}b_1" alt="b_1" title="b_1" />. So, <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7BdW_1%7D%20%3D%20%5Cfrac%7BdL%7D%7BdA_1%7D%20%2A%20%5Cfrac%7BdA_1%7D%7BdZ_1%7D%20%2A%20%5Cfrac%7BdZ_1%7D%7BdW_1%7D%2C" alt="\frac{dL}{dW_1} = \frac{dL}{dA_1} * \frac{dA_1}{dZ_1} * \frac{dZ_1}{dW_1}," title="\frac{dL}{dW_1} = \frac{dL}{dA_1} * \frac{dA_1}{dZ_1} * \frac{dZ_1}{dW_1}," /><br /> and <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}%5Cfrac%7BdL%7D%7Bdb_1%7D%20%3D%20%5Cfrac%7BdL%7D%7BdA_1%7D%20%2A%20%5Cfrac%7BdA_1%7D%7BdZ_1%7D%20%2A%20%5Cfrac%7BdZ_1%7D%7Bdb_1%7D." alt="\frac{dL}{db_1} = \frac{dL}{dA_1} * \frac{dA_1}{dZ_1} * \frac{dZ_1}{db_1}." title="\frac{dL}{db_1} = \frac{dL}{dA_1} * \frac{dA_1}{dZ_1} * \frac{dZ_1}{db_1}." /><br /></p>
<h2 id="gradient-descent">Gradient Descent</h2>
<p>For each of the obtained gradients, we now calculate the new weight: <br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cdpi{100}W%20%3D%20W%20-%20%5Calpha%2A%5Cfrac%7BdL%7D%7BdW%7D%2C" alt="W = W - \alpha*\frac{dL}{dW}," title="W = W - \alpha*\frac{dL}{dW}," /><br /> where alpha is a so called hyper parameter, which can’t be learned, but has to be provided at the beginning of training.</p>
<h2 id="live-example">Live example:</h2>
<p>View this NN in action: <a href="https://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=10&amp;dataset=gauss&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=1,1&amp;seed=0.43153&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">Tensorflow JS Playground</a></p>
